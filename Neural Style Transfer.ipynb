{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural Style Transfer.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNd1MOFC+rLh7/+Jx+H2fLu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yKnBXA9Ehg7O","colab_type":"text"},"source":["# Import the libraries"]},{"cell_type":"code","metadata":{"id":"UnFTgBWym5Ee","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":64},"outputId":"8f281f42-76c8-4270-8082-a85d8b7e6db9","executionInfo":{"status":"ok","timestamp":1581769002271,"user_tz":-345,"elapsed":2701,"user":{"displayName":"Saroj Bhattarai","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZcppI2PXDRMgj0rJzmb1tIfxm9RJsg5wLENCg6kw=s64","userId":"12946171905778839163"}}},"source":["\n","import tensorflow as tf"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"jdUNJwkJhJ9c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"511b9e19-0132-40b8-dc5c-c807d82730cd","executionInfo":{"status":"ok","timestamp":1581769002276,"user_tz":-345,"elapsed":2679,"user":{"displayName":"Saroj Bhattarai","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZcppI2PXDRMgj0rJzmb1tIfxm9RJsg5wLENCg6kw=s64","userId":"12946171905778839163"}}},"source":["tf.__version__"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.15.0'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"svwIVJySgxLA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a33c163b-e1f0-41cf-9d57-39260cb80309","executionInfo":{"status":"ok","timestamp":1581769002278,"user_tz":-345,"elapsed":2662,"user":{"displayName":"Saroj Bhattarai","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZcppI2PXDRMgj0rJzmb1tIfxm9RJsg5wLENCg6kw=s64","userId":"12946171905778839163"}}},"source":["from tensorflow.python.keras.preprocessing import image as kp_image\n","\n","# Keras is only used to load VGG19 model as a high level API to TensorFlow \n","from keras.applications.vgg19 import VGG19\n","from keras.models import Model\n","from keras import backend as K\n","\n","# pillow is used for loading and saving images\n","from PIL import Image\n","\n","# numPy is used for manipulation of array of object i.e Image in our case\n","import numpy as np"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"4_dKeR1xhowT","colab_type":"text"},"source":["# Initialize and helper Functions"]},{"cell_type":"code","metadata":{"id":"k_DVmNrhhteM","colab_type":"code","colab":{}},"source":["# list of layers to be considered for calculation of Content and Style Loss\n","content_layers = ['block3_conv3']\n","style_layers   = ['block1_conv1','block2_conv2','block4_conv3']\n","\n","num_content_layers = len(content_layers)\n","num_style_layers   = len(style_layers)\n","\n","# path where the content and style images are located\n","content_path = 'dancing.jpg'\n","style_path   = 'picasso.jpg'\n","\n","# Save the result as\n","save_name = 'generated.jpg'\n","\n","# path to where Vgg19 model weight is located \n","vgg_weights = \"vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZAnjkv5h132","colab_type":"code","colab":{}},"source":["def load_img(path_to_img):\n","  max_dim  = 512\n","  img      = Image.open(path_to_img)\n","  img_size = max(img.size)\n","  scale    = max_dim/img_size\n","  img      = img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), Image.ANTIALIAS)\n","  img      = kp_image.img_to_array(img)\n","\n","  # We need to broadcast the image array such that it has a batch dimension \n","  img = np.expand_dims(img, axis=0)\n","\n","  # preprocess raw images to make it suitable to be used by VGG19 model\n","  out = tf.keras.applications.vgg19.preprocess_input(img)\n","\n","  return tf.convert_to_tensor(out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"avG_XUjCh72d","colab_type":"code","colab":{}},"source":["def deprocess_img(processed_img):\n","  x = processed_img.copy()\n","  \n","  # perform the inverse of the preprocessiing step\n","  x[:, :, 0] += 103.939\n","  x[:, :, 1] += 116.779\n","  x[:, :, 2] += 123.68\n","\n","  x = x[:, :, ::-1]\n","  x = np.clip(x, 0, 255).astype('uint8')\n","\n","  return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O2n12ilGiBYQ","colab_type":"text"},"source":["# Loss Functions"]},{"cell_type":"code","metadata":{"id":"syef_k-QiDuY","colab_type":"code","colab":{}},"source":["### Content Loss Function\n","def get_content_loss(content, target):\n","  return tf.reduce_mean(tf.square(content - target)) /2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eGj-rOq9iHlb","colab_type":"code","colab":{}},"source":["### Style Loss Fucntion\n","def gram_matrix(input_tensor):\n","\n","  # if input tensor is a 3D array of size Nh x Nw X Nc\n","  # we reshape it to a 2D array of Nc x (Nh*Nw)\n","  channels = int(input_tensor.shape[-1])\n","  a = tf.reshape(input_tensor, [-1, channels])\n","  n = tf.shape(a)[0]\n","\n","  # get gram matrix \n","  gram = tf.matmul(a, a, transpose_a=True) \n","  return gram\n","\n","def get_style_loss(base_style, gram_target):\n","\n","  height, width, channels = base_style.get_shape().as_list()\n","  gram_style = gram_matrix(base_style)\n","  \n","  # Original eqn as a constant to divide i.e 1/(4. * (channels ** 2) * (width * height) ** 2)\n","  return tf.reduce_mean(tf.square(gram_style - gram_target)) / (channels**2 * width * height) #(4.0 * (channels ** 2) * (width * height) ** 2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NaqzJ9R9iLg2","colab_type":"code","colab":{}},"source":["### Use to pass content and style image through it \n","def get_feature_representations(model, content_path, style_path, num_content_layers):\n","\n","  # Load our images in \n","  content_image = load_img(content_path)\n","  style_image   = load_img(style_path)\n","  \n","  # batch compute content and style features\n","  content_outputs = model(content_image)\n","  style_outputs   = model(style_image)\n","  \n","  # Get the style and content feature representations from our model  \n","  style_features   = [ style_layer[0]  for style_layer    in style_outputs[num_content_layers:] ]\n","  content_features = [ content_layer[0] for content_layer in content_outputs[:num_content_layers] ]\n","\n","  return style_features, content_features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdCblBzriOZa","colab_type":"code","colab":{}},"source":["### Total Loss\n","def compute_loss(model, loss_weights, generated_output_activations, gram_style_features, content_features, num_content_layers, num_style_layers):\n","\n","  generated_content_activations = generated_output_activations[:num_content_layers]\n","  generated_style_activations   = generated_output_activations[num_content_layers:]\n","\n","  style_weight, content_weight = loss_weights\n","  \n","  style_score = 0\n","  content_score = 0\n","\n","  # Accumulate style losses from all layers\n","  # Here, we equally weight each contribution of each loss layer\n","  weight_per_style_layer = 1.0 / float(num_style_layers)\n","  for target_style, comb_style in zip(gram_style_features, generated_style_activations):\n","    temp = get_style_loss(comb_style[0], target_style)\n","    style_score += weight_per_style_layer * temp\n","    \n","  # Accumulate content losses from all layers \n","  weight_per_content_layer = 1.0 / float(num_content_layers)\n","  for target_content, comb_content in zip(content_features, generated_content_activations):\n","    temp = get_content_loss(comb_content[0], target_content)\n","    content_score += weight_per_content_layer* temp\n","\n","  # Get total loss\n","  loss = style_weight*style_score + content_weight*content_score \n","\n","\n","  return loss, style_score, content_score\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yoVfnq6NiTD0","colab_type":"code","colab":{}},"source":["# Using Keras Load VGG19 model\n","def get_model(content_layers,style_layers):\n","\n","  # Load our model. We load pretrained VGG, trained on imagenet data\n","  vgg19 = VGG19(weights=None, include_top=False)\n","  # We don't need to (or want to) train any layers of our pre-trained vgg model, so we set it's trainable to false.\n","  vgg19.trainable = False\n","\n","  style_model_outputs   =  [vgg19.get_layer(name).output for name in style_layers]\n","  content_model_outputs =  [vgg19.get_layer(name).output for name in content_layers]\n","  \n","  model_outputs = content_model_outputs + style_model_outputs\n","\n","  # Build model \n","  return Model(inputs = vgg19.input, outputs = model_outputs),  vgg19"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3aqUnqhEiXwQ","colab_type":"code","colab":{}},"source":["def run_style_transfer(content_path, style_path, num_iterations=200, content_weight=0.1, style_weight=0.9): \n","\n","  # Create a tensorflow session \n","  sess = tf.Session()\n","\n","  # Assign keras back-end to the TF session which we created\n","  K.set_session(sess)\n","\n","  model, vgg19 = get_model(content_layers,style_layers)\n","\n","  # Get the style and content feature representations (from our specified intermediate layers) \n","  style_features, content_features = get_feature_representations(model, content_path, style_path, num_content_layers)\n","  gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n","\n","  # VGG default normalization\n","  norm_means = np.array([103.939, 116.779, 123.68])\n","  min_vals = -norm_means\n","  max_vals = 255 - norm_means \n","    \n","\n","  # In original paper, the initial stylized image is random matrix of same size as that of content image\n","  # but in later images content image was used instead on random values for first stylized image\n","  # because it proved to help to stylize faster\n","  generated_image = load_img(content_path)\n","  # generated_image = np.random.randint(0,255, size=generated_image.shape) \n","  \n","  # Create tensorflow variable to hold a stylized/generated image during the training \n","  generated_image = tf.Variable(generated_image, dtype=tf.float32)\n","\n","  model_outputs = model(generated_image)\n","\n","  # weightages of each content and style images i.e alpha & beta\n","  loss_weights = (style_weight, content_weight)\n","\n","  # Create our optimizer\n","  loss = compute_loss(model, loss_weights, model_outputs, gram_style_features, content_features, num_content_layers, num_style_layers)\n","  opt = tf.train.AdamOptimizer(learning_rate=9, beta1=0.9, epsilon=1e-1).minimize( loss[0], var_list = [generated_image])\n","\n","  sess.run(tf.global_variables_initializer())\n","  sess.run(generated_image.initializer)\n","  \n","  # loading the weights again because tf.global_variables_initializer() resets the weights\n","  vgg19.load_weights(vgg_weights)\n","\n","\n","  # Put loss as infinity before training starts and Create a variable to hold best image (i.e image with minimum loss)\n","  best_loss, best_img = float('inf'), None\n","\n","  for i in range(num_iterations):\n","\n","    # Do optimization\n","    sess.run(opt)\n","\n","    # Make sure image values stays in the range of max-min value of VGG norm \n","    clipped = tf.clip_by_value(generated_image, min_vals, max_vals)\n","    # assign the clipped value to the tensor stylized image\n","    generated_image.assign(clipped)\n","\n","\n","    # Open the Tuple of tensors \n","    total_loss, style_score, content_score = loss\n","    total_loss = total_loss.eval(session=sess)\n","\n","\n","    if total_loss < best_loss:\n","\n","      # Update best loss and best image from total loss. \n","      best_loss = total_loss\n","\n","      # generated image is of shape (1, h, w, 3) convert it to (h, w, 3)\n","      temp_generated_image = sess.run(generated_image)[0]\n","      best_img = deprocess_img(temp_generated_image)\n","\n","      s_loss = sess.run(style_score)\n","      c_loss = sess.run(content_score)\n","\n","      # print best loss\n","      print('best: iteration: ', i ,'loss: ', total_loss ,'  style_loss: ',  s_loss,'  content_loss: ', c_loss)\n","\n","    # Save image after every 100 iterations \n","    if (i+1)%100 == 0:\n","      output = Image.fromarray(best_img)\n","      output.save(str(i+1)+'-'+save_name)\n","\n","  # after num_iterations iterations are completed, close the TF session \n","  sess.close()\n","      \n","  return best_img, best_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKLp4rGQigTV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d7e42fe4-e071-4d58-f6cc-602ace6dd0cd","executionInfo":{"status":"ok","timestamp":1581770532425,"user_tz":-345,"elapsed":92072,"user":{"displayName":"Saroj Bhattarai","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZcppI2PXDRMgj0rJzmb1tIfxm9RJsg5wLENCg6kw=s64","userId":"12946171905778839163"}}},"source":["best, best_loss = run_style_transfer(content_path, style_path, num_iterations=200, content_weight=0.1, style_weight=0.9)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","best: iteration:  0 loss:  35113500.0   style_loss:  39013156.0   content_loss:  16622.217\n","best: iteration:  1 loss:  31993596.0   style_loss:  35546370.0   content_loss:  18658.664\n","best: iteration:  2 loss:  21476928.0   style_loss:  23861360.0   content_loss:  17031.715\n","best: iteration:  3 loss:  16614915.0   style_loss:  18459202.0   content_loss:  16323.823\n","best: iteration:  4 loss:  13318447.0   style_loss:  14796335.0   content_loss:  17456.895\n","best: iteration:  5 loss:  10285743.0   style_loss:  11426548.0   content_loss:  18504.564\n","best: iteration:  6 loss:  8041182.0   style_loss:  8932560.0   content_loss:  18781.02\n","best: iteration:  7 loss:  6631694.5   style_loss:  7366411.5   content_loss:  19243.348\n","best: iteration:  8 loss:  4929968.5   style_loss:  5475540.0   content_loss:  19823.283\n","best: iteration:  9 loss:  3906275.2   style_loss:  4338036.5   content_loss:  20425.156\n","best: iteration:  10 loss:  3078847.8   style_loss:  3418628.0   content_loss:  20828.373\n","best: iteration:  11 loss:  2585253.2   style_loss:  2870143.5   content_loss:  21239.877\n","best: iteration:  12 loss:  2110316.2   style_loss:  2342389.0   content_loss:  21661.945\n","best: iteration:  13 loss:  1895080.9   style_loss:  2103192.5   content_loss:  22076.514\n","best: iteration:  14 loss:  1741884.0   style_loss:  1932939.5   content_loss:  22384.758\n","best: iteration:  15 loss:  1707065.8   style_loss:  1894225.5   content_loss:  22630.13\n","best: iteration:  16 loss:  1640340.5   style_loss:  1820065.0   content_loss:  22819.535\n","best: iteration:  17 loss:  1637440.8   style_loss:  1816825.1   content_loss:  22981.932\n","best: iteration:  18 loss:  1597961.2   style_loss:  1772947.4   content_loss:  23086.352\n","best: iteration:  19 loss:  1566613.8   style_loss:  1738104.9   content_loss:  23193.422\n","best: iteration:  20 loss:  1472356.5   style_loss:  1633359.0   content_loss:  23335.203\n","best: iteration:  21 loss:  1371866.5   style_loss:  1521684.1   content_loss:  23507.56\n","best: iteration:  22 loss:  1233780.0   style_loss:  1368235.8   content_loss:  23678.488\n","best: iteration:  23 loss:  1103606.8   style_loss:  1223579.6   content_loss:  23851.545\n","best: iteration:  24 loss:  956840.7   style_loss:  1060489.2   content_loss:  24003.49\n","best: iteration:  25 loss:  828305.0   style_loss:  917661.4   content_loss:  24098.324\n","best: iteration:  26 loss:  702738.9   style_loss:  778140.1   content_loss:  24127.332\n","best: iteration:  27 loss:  606822.0   style_loss:  671563.25   content_loss:  24150.885\n","best: iteration:  28 loss:  523987.28   style_loss:  579517.56   content_loss:  24214.277\n","best: iteration:  29 loss:  469897.53   style_loss:  519409.1   content_loss:  24293.352\n","best: iteration:  30 loss:  426873.4   style_loss:  471598.94   content_loss:  24343.846\n","best: iteration:  31 loss:  403425.4   style_loss:  445541.0   content_loss:  24385.297\n","best: iteration:  32 loss:  384716.8   style_loss:  424747.7   content_loss:  24439.188\n","best: iteration:  33 loss:  377014.6   style_loss:  416183.22   content_loss:  24497.277\n","best: iteration:  34 loss:  367873.4   style_loss:  406021.12   content_loss:  24544.287\n","best: iteration:  35 loss:  361507.03   style_loss:  398940.8   content_loss:  24603.254\n","best: iteration:  36 loss:  350209.16   style_loss:  386378.66   content_loss:  24683.406\n","best: iteration:  37 loss:  339097.8   style_loss:  374024.34   content_loss:  24759.182\n","best: iteration:  38 loss:  324321.1   style_loss:  357598.94   content_loss:  24820.486\n","best: iteration:  39 loss:  308754.0   style_loss:  340294.38   content_loss:  24890.547\n","best: iteration:  40 loss:  291494.88   style_loss:  321109.25   content_loss:  24965.473\n","best: iteration:  41 loss:  273637.25   style_loss:  301261.28   content_loss:  25020.49\n","best: iteration:  42 loss:  256866.0   style_loss:  282621.38   content_loss:  25067.697\n","best: iteration:  43 loss:  240220.4   style_loss:  264118.75   content_loss:  25135.13\n","best: iteration:  44 loss:  226317.77   style_loss:  248663.33   content_loss:  25207.611\n","best: iteration:  45 loss:  212852.14   style_loss:  233695.69   content_loss:  25260.424\n","best: iteration:  46 loss:  202316.1   style_loss:  221982.9   content_loss:  25314.92\n","best: iteration:  47 loss:  192678.12   style_loss:  211266.2   content_loss:  25385.262\n","best: iteration:  48 loss:  185009.55   style_loss:  202738.19   content_loss:  25452.078\n","best: iteration:  49 loss:  178446.53   style_loss:  195439.95   content_loss:  25506.006\n","best: iteration:  50 loss:  172579.98   style_loss:  188915.5   content_loss:  25560.523\n","best: iteration:  51 loss:  167909.48   style_loss:  183720.55   content_loss:  25610.072\n","best: iteration:  52 loss:  163039.23   style_loss:  178305.56   content_loss:  25642.424\n","best: iteration:  53 loss:  158869.19   style_loss:  173668.61   content_loss:  25674.322\n","best: iteration:  54 loss:  154507.67   style_loss:  168818.06   content_loss:  25714.225\n","best: iteration:  55 loss:  150221.73   style_loss:  164052.16   content_loss:  25747.91\n","best: iteration:  56 loss:  146150.06   style_loss:  159524.88   content_loss:  25776.654\n","best: iteration:  57 loss:  141827.92   style_loss:  154719.06   content_loss:  25807.713\n","best: iteration:  58 loss:  137955.02   style_loss:  150413.58   content_loss:  25827.94\n","best: iteration:  59 loss:  134125.12   style_loss:  146157.17   content_loss:  25836.75\n","best: iteration:  60 loss:  130477.33   style_loss:  142102.17   content_loss:  25853.77\n","best: iteration:  61 loss:  127260.016   style_loss:  138525.03   content_loss:  25874.938\n","best: iteration:  62 loss:  124100.81   style_loss:  135013.69   content_loss:  25885.0\n","best: iteration:  63 loss:  121254.38   style_loss:  131850.12   content_loss:  25892.77\n","best: iteration:  64 loss:  118616.59   style_loss:  128918.086   content_loss:  25903.178\n","best: iteration:  65 loss:  116017.74   style_loss:  126029.66   content_loss:  25910.637\n","best: iteration:  66 loss:  113679.66   style_loss:  123430.31   content_loss:  25923.777\n","best: iteration:  67 loss:  111410.09   style_loss:  120906.34   content_loss:  25943.871\n","best: iteration:  68 loss:  109191.92   style_loss:  118439.945   content_loss:  25959.84\n","best: iteration:  69 loss:  107161.18   style_loss:  116181.83   content_loss:  25975.355\n","best: iteration:  70 loss:  105170.586   style_loss:  113968.2   content_loss:  25992.05\n","best: iteration:  71 loss:  103229.0   style_loss:  111809.875   content_loss:  26001.143\n","best: iteration:  72 loss:  101414.164   style_loss:  109792.375   content_loss:  26010.281\n","best: iteration:  73 loss:  99616.19   style_loss:  107792.77   content_loss:  26026.953\n","best: iteration:  74 loss:  97859.43   style_loss:  105839.12   content_loss:  26042.287\n","best: iteration:  75 loss:  96199.24   style_loss:  103992.875   content_loss:  26056.652\n","best: iteration:  76 loss:  94565.98   style_loss:  102176.31   content_loss:  26072.994\n","best: iteration:  77 loss:  92963.22   style_loss:  100394.09   content_loss:  26085.434\n","best: iteration:  78 loss:  91447.12   style_loss:  98708.234   content_loss:  26097.115\n","best: iteration:  79 loss:  89973.16   style_loss:  97069.11   content_loss:  26109.62\n","best: iteration:  80 loss:  88527.555   style_loss:  95461.85   content_loss:  26119.006\n","best: iteration:  81 loss:  87150.97   style_loss:  93930.94   content_loss:  26131.256\n","best: iteration:  82 loss:  85830.43   style_loss:  92462.16   content_loss:  26144.996\n","best: iteration:  83 loss:  84531.25   style_loss:  91017.734   content_loss:  26152.793\n","best: iteration:  84 loss:  83275.25   style_loss:  89621.44   content_loss:  26159.596\n","best: iteration:  85 loss:  82070.375   style_loss:  88281.984   content_loss:  26165.959\n","best: iteration:  86 loss:  80890.3   style_loss:  86970.3   content_loss:  26170.305\n","best: iteration:  87 loss:  79730.54   style_loss:  85680.77   content_loss:  26178.525\n","best: iteration:  88 loss:  78609.53   style_loss:  84434.19   content_loss:  26187.625\n","best: iteration:  89 loss:  77518.45   style_loss:  83221.05   content_loss:  26195.154\n","best: iteration:  90 loss:  76442.94   style_loss:  82025.234   content_loss:  26202.285\n","best: iteration:  91 loss:  75389.266   style_loss:  80854.16   content_loss:  26205.236\n","best: iteration:  92 loss:  74366.95   style_loss:  79717.83   content_loss:  26209.082\n","best: iteration:  93 loss:  73369.52   style_loss:  78608.76   content_loss:  26216.314\n","best: iteration:  94 loss:  72390.26   style_loss:  77519.98   content_loss:  26222.729\n","best: iteration:  95 loss:  71434.09   style_loss:  76456.7   content_loss:  26230.594\n","best: iteration:  96 loss:  70505.29   style_loss:  75423.95   content_loss:  26237.314\n","best: iteration:  97 loss:  69600.336   style_loss:  74417.93   content_loss:  26242.053\n","best: iteration:  98 loss:  68712.44   style_loss:  73430.7   content_loss:  26248.043\n","best: iteration:  99 loss:  67841.82   style_loss:  72462.71   content_loss:  26253.816\n","best: iteration:  100 loss:  66991.65   style_loss:  71517.17   content_loss:  26261.99\n","best: iteration:  101 loss:  66161.03   style_loss:  70593.375   content_loss:  26269.982\n","best: iteration:  102 loss:  65346.5   style_loss:  69687.74   content_loss:  26275.285\n","best: iteration:  103 loss:  64546.83   style_loss:  68798.55   content_loss:  26281.361\n","best: iteration:  104 loss:  63762.363   style_loss:  67926.38   content_loss:  26286.227\n","best: iteration:  105 loss:  62994.52   style_loss:  67072.625   content_loss:  26291.586\n","best: iteration:  106 loss:  62242.953   style_loss:  66236.94   content_loss:  26297.11\n","best: iteration:  107 loss:  61506.027   style_loss:  65417.586   content_loss:  26302.021\n","best: iteration:  108 loss:  60782.137   style_loss:  64612.566   content_loss:  26308.312\n","best: iteration:  109 loss:  60071.133   style_loss:  63822.008   content_loss:  26313.264\n","best: iteration:  110 loss:  59373.55   style_loss:  63046.273   content_loss:  26319.062\n","best: iteration:  111 loss:  58689.43   style_loss:  62285.555   content_loss:  26324.318\n","best: iteration:  112 loss:  58018.316   style_loss:  61539.37   content_loss:  26328.842\n","best: iteration:  113 loss:  57359.465   style_loss:  60806.74   content_loss:  26333.982\n","best: iteration:  114 loss:  56712.12   style_loss:  60086.938   content_loss:  26338.78\n","best: iteration:  115 loss:  56076.14   style_loss:  59379.613   content_loss:  26344.89\n","best: iteration:  116 loss:  55451.164   style_loss:  58684.65   content_loss:  26349.824\n","best: iteration:  117 loss:  54837.395   style_loss:  58002.05   content_loss:  26355.453\n","best: iteration:  118 loss:  54234.633   style_loss:  57331.77   content_loss:  26360.375\n","best: iteration:  119 loss:  53642.438   style_loss:  56673.234   content_loss:  26365.312\n","best: iteration:  120 loss:  53060.59   style_loss:  56026.19   content_loss:  26370.223\n","best: iteration:  121 loss:  52488.65   style_loss:  55390.18   content_loss:  26374.871\n","best: iteration:  122 loss:  51926.586   style_loss:  54765.14   content_loss:  26379.625\n","best: iteration:  123 loss:  51374.117   style_loss:  54150.86   content_loss:  26383.496\n","best: iteration:  124 loss:  50830.78   style_loss:  53546.61   content_loss:  26388.377\n","best: iteration:  125 loss:  50296.582   style_loss:  52952.64   content_loss:  26392.07\n","best: iteration:  126 loss:  49771.43   style_loss:  52368.64   content_loss:  26396.55\n","best: iteration:  127 loss:  49255.168   style_loss:  51794.688   content_loss:  26399.525\n","best: iteration:  128 loss:  48747.906   style_loss:  51230.508   content_loss:  26404.47\n","best: iteration:  129 loss:  48250.105   style_loss:  50677.1   content_loss:  26407.158\n","best: iteration:  130 loss:  47762.562   style_loss:  50134.797   content_loss:  26412.453\n","best: iteration:  131 loss:  47287.28   style_loss:  49606.5   content_loss:  26414.355\n","best: iteration:  132 loss:  46828.555   style_loss:  49096.062   content_loss:  26420.965\n","best: iteration:  133 loss:  46396.793   style_loss:  48616.316   content_loss:  26421.068\n","best: iteration:  134 loss:  46015.375   style_loss:  48191.5   content_loss:  26430.254\n","best: iteration:  135 loss:  45740.0   style_loss:  47885.98   content_loss:  26426.156\n","best: iteration:  136 loss:  45698.668   style_loss:  47838.387   content_loss:  26441.21\n","best: iteration:  154 loss:  42867.656   style_loss:  44695.35   content_loss:  26418.395\n","best: iteration:  161 loss:  40509.777   style_loss:  42072.926   content_loss:  26441.441\n","best: iteration:  164 loss:  39611.633   style_loss:  41079.31   content_loss:  26402.582\n","best: iteration:  168 loss:  39564.543   style_loss:  41017.96   content_loss:  26483.773\n","best: iteration:  171 loss:  35930.043   style_loss:  36979.83   content_loss:  26482.008\n","best: iteration:  174 loss:  35847.52   style_loss:  36890.004   content_loss:  26465.166\n","best: iteration:  178 loss:  34176.367   style_loss:  35027.57   content_loss:  26515.537\n","best: iteration:  181 loss:  32817.04   style_loss:  33518.566   content_loss:  26503.293\n","best: iteration:  185 loss:  31853.152   style_loss:  32443.324   content_loss:  26541.621\n","best: iteration:  188 loss:  31290.041   style_loss:  31815.234   content_loss:  26563.303\n","best: iteration:  189 loss:  30384.402   style_loss:  30811.436   content_loss:  26541.104\n","best: iteration:  192 loss:  29728.232   style_loss:  30081.105   content_loss:  26552.365\n","best: iteration:  193 loss:  29606.03   style_loss:  29948.527   content_loss:  26523.56\n","best: iteration:  196 loss:  28827.27   style_loss:  29080.414   content_loss:  26548.982\n","best: iteration:  197 loss:  28308.633   style_loss:  28503.188   content_loss:  26557.662\n"],"name":"stdout"}]}]}